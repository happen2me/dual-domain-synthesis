{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6158d928-7f55-4326-a462-ded7c7eba7de",
   "metadata": {},
   "source": [
    "# Dual Domain Synthesis\n",
    "\n",
    "- [github](https://github.com/denabazazian/Dual-Domain-Synthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f1596-d2f7-4d98-a87c-a78da3f1f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/extra/micheal/dd_synthesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2e97e-3c33-47b6-86dc-6b2c8069c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import Namespace\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "from utils_repurpose import tensor2image\n",
    "from perceptual_model import VGG16_for_Perceptual\n",
    "from stylegan2 import Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111db20-5b32-4c5b-84b7-c1e6efdc5ff7",
   "metadata": {},
   "source": [
    "## Step 1. Optimize a Dual-domain Latent\n",
    "\n",
    "Here I'll optimize a latent so that it generates images similar to OCT and iOCT with respective generators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2acdf-c00a-4784-81c7-80fdaa5fc55b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0e0e7-5826-4c58-8622-876db8ce2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_path = \"checkpoints/aroi_to_op/036000.pt\"\n",
    "target_model_path = \"submodules/stylegan2-pytorch/checkpoint/030000.pt\" # OCT (AROI)\n",
    "save_path_root = \"artifacts/op2ioct/results\"\n",
    "source_image_path = \"/home/extra/micheal/pixel2style2pixel/data/ioct/bscans/train/OS-2020-02-03_135647fs-073.png\"\n",
    "source_mask_path = \"/home/extra/micheal/pixel2style2pixel/data/ioct/labels/train/OS-2020-02-03_135647fs-073.png\"\n",
    "target_image_path = \"/home/extra/micheal/IDP/data/splits/AROI/original/bscans/train/patient15_raw0025.png\"\n",
    "target_mask_path = \"/home/extra/micheal/IDP/data/splits/AROI/original/labels/train/patient15_raw0025.png\"\n",
    "image_size = 256\n",
    "n_samples = 1\n",
    "imshow_size = 256\n",
    "latent_dim = 512\n",
    "truncation = 0.7\n",
    "n_test = 1\n",
    "id_dir = \"mix\"\n",
    "sample_z_path = \"artifacts/op2ioct/latents\"\n",
    "save_iterations = True\n",
    "mask_guided_iterations = 1002\n",
    "lr = 0.01\n",
    "n_mean_latent = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c83b8-34a0-42e1-bd91-e6eaf6e572d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b628d0e-0cf6-47b0-a304-5181c8a99bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def concat_features(features):\n",
    "    h = max([f.shape[-2] for f in features])\n",
    "    w = max([f.shape[-1] for f in features])\n",
    "    return torch.cat([torch.nn.functional.interpolate(f, (h, w), mode='nearest') for f in features], dim=1)\n",
    "\n",
    "class Convert2Uint8(torch.nn.Module):\n",
    "    '''\n",
    "    Resize input when the target dim is not divisible by the input dim\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image or Tensor): Image to be scaled.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor: Rescaled image.\n",
    "        \"\"\"\n",
    "        img = torch.round(torch.mul(img, 255))\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToOneHot(torch.nn.Module):\n",
    "    '''\n",
    "    Convert input to one-hot encoding\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Image to be scaled of shape (1, h, w).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Rescaled image.\n",
    "        \"\"\"\n",
    "        img = img.long()[0]\n",
    "        # img = torch.nn.functional.one_hot(img, num_classes=self.num_classes)\n",
    "        img = img.permute(2, 0, 1)\n",
    "        return img\n",
    "\n",
    "\n",
    "class MapVal(torch.nn.Module):\n",
    "    '''\n",
    "    Map a list of value to another\n",
    "    '''\n",
    "\n",
    "    def __init__(self, src_vals, dst_vals):\n",
    "        super().__init__()\n",
    "        assert len(src_vals) == len(\n",
    "            dst_vals), \"src_vals and dst_vals must of equal length\"\n",
    "        self.src_vals = src_vals\n",
    "        self.dst_vals = dst_vals\n",
    "\n",
    "    def forward(self, img):\n",
    "        for s, d in zip(self.src_vals, self.dst_vals):\n",
    "            img[img == s] = d\n",
    "        return img\n",
    "\n",
    "\n",
    "def get_transforms(opts):\n",
    "    transforms_dict = {\n",
    "        'transform_gt_train': transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5] * opts.output_nc, [0.5] * opts.output_nc)]),\n",
    "        'transform_source': transforms.Compose([\n",
    "            transforms.Resize(\n",
    "                (256, 256), interpolation=InterpolationMode.NEAREST),\n",
    "            transforms.ToTensor(),\n",
    "            Convert2Uint8(),\n",
    "            MapVal(opts.src_vals, opts.dst_vals),\n",
    "            # ToOneHot(opts.label_nc)\n",
    "        ])\n",
    "    }\n",
    "    return transforms_dict\n",
    "\n",
    "\n",
    "def load_bscan(bscan_path):\n",
    "    \"\"\"Load and add a batch dimension\n",
    "    \"\"\"\n",
    "    image = Image.open(bscan_path).convert('RGB')\n",
    "    image = bscan_transform(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_label(label_path):\n",
    "    \"\"\"Load and add a batch dimension\n",
    "    \"\"\"\n",
    "    image = Image.open(label_path).convert('L')\n",
    "    image = label_transform(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def caluclate_loss(synth_img, img, perceptual_net, mask, MSE_Loss, image_resolution):\n",
    "\n",
    "    img_p = torch.nn.Upsample(scale_factor=(\n",
    "        256/image_resolution), mode='bilinear')(img)\n",
    "    real_0, real_1, real_2, real_3 = perceptual_net(img_p)\n",
    "    synth_p = torch.nn.Upsample(scale_factor=(\n",
    "        256/image_resolution), mode='bilinear')(synth_img)\n",
    "    synth_0, synth_1, synth_2, synth_3 = perceptual_net(synth_p)\n",
    "\n",
    "    perceptual_loss = 0\n",
    "    mask = torch.nn.Upsample(scale_factor=(\n",
    "        256/image_resolution), mode='bilinear')(mask)\n",
    "    perceptual_loss += MSE_Loss(synth_0*mask.expand(1,\n",
    "                                64, 256, 256), real_0*mask.expand(1, 64, 256, 256))\n",
    "    perceptual_loss += MSE_Loss(synth_1*mask.expand(1,\n",
    "                                64, 256, 256), real_1*mask.expand(1, 64, 256, 256))\n",
    "    mask = torch.nn.Upsample(scale_factor=(64/256), mode='bilinear')(mask)\n",
    "    perceptual_loss += MSE_Loss(synth_2*mask.expand(1,\n",
    "                                256, 64, 64), real_2*mask.expand(1, 256, 64, 64))\n",
    "    mask = torch.nn.Upsample(scale_factor=(32/64), mode='bilinear')(mask)\n",
    "    perceptual_loss += MSE_Loss(synth_3*mask.expand(1,\n",
    "                                512, 32, 32), real_3*mask.expand(1, 512, 32, 32))\n",
    "\n",
    "    return perceptual_loss\n",
    "\n",
    "\n",
    "def noise_normalize_(noises):\n",
    "    for noise in noises:\n",
    "        mean = noise.mean()\n",
    "        std = noise.std()\n",
    "\n",
    "        noise.data.add_(-mean).div_(std)\n",
    "\n",
    "\n",
    "def horizontal_expand(label, feature, to_expand=20):\n",
    "    if isinstance(label, torch.Tensor):\n",
    "        label_copy = label.clone()\n",
    "    else:\n",
    "        label_copy = label.copy()\n",
    "    x, y = np.where(label_copy == feature)\n",
    "    xacc, yacc = x, y\n",
    "    for i in range(to_expand):\n",
    "        xacc = np.concatenate([xacc, x])\n",
    "        yacc = np.concatenate([yacc, y+i])\n",
    "    label_copy[xacc, yacc] = feature\n",
    "    return label_copy\n",
    "\n",
    "\n",
    "def expand_label(label, instrument_label=2, shadow_label=4, expansion_instrument=30,\n",
    "                 expansion_shadow=60):\n",
    "    \"\"\"The input size is expected to be [h, w]\n",
    "    \"\"\"\n",
    "    # For label 2 4 (instrument & its mirroring), we horizontally expand\n",
    "    # a couple of pixels rightward\n",
    "    label = horizontal_expand(label, instrument_label,\n",
    "                              to_expand=expansion_instrument)\n",
    "    # shadows are generally broader\n",
    "    label = horizontal_expand(label, shadow_label, to_expand=expansion_shadow)\n",
    "    return label\n",
    "\n",
    "\n",
    "def get_shadow(label, instrument_label=2, shadow_label=4, top_layer_label=1, img_width=256, img_height=256):\n",
    "    shadow_x = np.array([], dtype=np.int64)\n",
    "    shadow_y = np.array([], dtype=np.int64)\n",
    "    # Requirements for the shadow label:\n",
    "    # 1. Horizontally after the starting of the instrument/mirroring & before the\n",
    "    #    ending of the instrument/mirroring\n",
    "    # 2. Vertically below the lower bound of instrument/mirroring\n",
    "    x, y = np.where(np.logical_or(label==instrument_label, label==shadow_label)) # (1024, 512)\n",
    "    if len(x) == 0:\n",
    "        return shadow_x, shadow_y\n",
    "    left_bound = np.min(y)\n",
    "    right_bound = np.max(y)\n",
    "    accumulated_min_lowerbound = 0\n",
    "    for i in range(left_bound, right_bound):\n",
    "        instrument_above = np.where(np.logical_or(label[:, i] == instrument_label, label[:, i] == shadow_label))[0]\n",
    "        if len(instrument_above) == 0:\n",
    "            if accumulated_min_lowerbound == 0:\n",
    "                continue\n",
    "            else:\n",
    "                # set to current recorded lowest shadow\n",
    "                instrument_lowerbound = accumulated_min_lowerbound\n",
    "        else:\n",
    "            # print(\"instrument_above\", instrument_above, len(instrument_above))\n",
    "            instrument_lowerbound = np.max(instrument_above)\n",
    "            if accumulated_min_lowerbound == 0:\n",
    "                # initialize\n",
    "                accumulated_min_lowerbound = instrument_lowerbound\n",
    "            else:\n",
    "                accumulated_min_lowerbound = max(accumulated_min_lowerbound, instrument_lowerbound)\n",
    "        x_vertical = np.arange(instrument_lowerbound, img_height) # upperbound to bottom\n",
    "        y_vertical = np.full_like(x_vertical, i)\n",
    "        shadow_x = np.concatenate([shadow_x, x_vertical])\n",
    "        shadow_y = np.concatenate([shadow_y, y_vertical])\n",
    "    return shadow_x, shadow_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87746e4-bf45-4f6d-9332-bccf7864ac69",
   "metadata": {},
   "source": [
    "### 2. Load Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95097993-8e67-4af8-9341-dc9bc42f500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "generator = Generator(image_size, latent_dim, 8)\n",
    "generator_ckpt = torch.load(generator_path, map_location='cpu')\n",
    "generator.load_state_dict(generator_ckpt[\"g_ema\"], strict=False)\n",
    "generator = generator.eval().to(device)\n",
    "\n",
    "targ_generator = Generator(image_size, latent_dim, 8).to(device)\n",
    "targ_generator = nn.parallel.DataParallel(targ_generator)\n",
    "\n",
    "targ_generator_ckpt = torch.load(target_model_path)\n",
    "targ_generator.load_state_dict(targ_generator_ckpt[\"g_ema\"], strict=False)\n",
    "targ_generator = targ_generator.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a82ba6-5355-42eb-937e-ededd8bf86f4",
   "metadata": {},
   "source": [
    "### 3. Load bscan & label pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ba5d0-bec2-4693-b933-9636f93e2880",
   "metadata": {},
   "source": [
    "Load source & target bscan and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98145d-d77b-4ab0-bfaa-d10a6d0e38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AROI_LABELS = [19, 57, 171, 190]\n",
    "INSTRUMENT_LABELS = [100, 200]\n",
    "instrument_map = [5, 6]\n",
    "FLUID_LABELS = [80, 160, 240]\n",
    "transform_dict = get_transforms(Namespace(\n",
    "    output_nc=3, label_nc=2, src_vals=[2, 3, 4]+AROI_LABELS+INSTRUMENT_LABELS+FLUID_LABELS,\n",
    "    dst_vals=[5, 2, 6]+[1, 2, 3, 4]+instrument_map+[0, 0, 0]))\n",
    "bscan_transform = transform_dict['transform_gt_train']\n",
    "label_transform = transform_dict['transform_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291918a6-4b3c-44e0-8606-5a3854f2021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_gen = load_bscan(source_image_path)  # # source is iOCT, [3, h, w]\n",
    "masks = load_label(source_mask_path)  # assume to be [1, h, w]\n",
    "targ_imgs_gen = load_bscan(target_image_path)  # target is OCT\n",
    "targ_masks = load_label(target_mask_path)\n",
    "\n",
    "print(f\"image shape {list(imgs_gen.shape)}, mask shape {list(masks.shape)}\")\n",
    "print(np.unique(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da9d98-8249-4bd4-90fc-191c830b35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_source = imgs_gen.unsqueeze(0).to(\n",
    "    device)  # (1,3,image_size,image_size) (1,3,256,256)\n",
    "img_target = targ_imgs_gen.unsqueeze(0).to(\n",
    "    device)  # (1,3,image_size,image_size) (1,3,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245fbc39-9495-42ef-aa60-50e85fb4396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize source and target image\n",
    "fig, axes = plt.subplots(1, 2, figsize=(5, 10))\n",
    "axes[0].imshow(Image.open(source_image_path))\n",
    "axes[1].imshow(Image.open(target_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be45f79-62cb-4c56-951a-dd63f1c9dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize source and target mask\n",
    "fig, axes = plt.subplots(1, 2, figsize=(5, 10))\n",
    "axes[0].imshow(masks[0])\n",
    "axes[1].imshow(targ_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb92ee5-b368-44ac-82a1-bd37f3d9a30f",
   "metadata": {},
   "source": [
    "### 4. Create cross-over masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1698e97-b848-415c-bc7e-957de0e422b8",
   "metadata": {},
   "source": [
    "expand instrumen|ts and shadows in source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50a533-6384-4b2a-a30b-aa03b4e885b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = expand_label(masks[0, :, :], instrument_label=5, shadow_label=6,\n",
    "                    expansion_instrument=15, expansion_shadow=15)  # (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e617b-3c2f-4d65-97bd-86a4ec8f23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select classes of interest (instrument, its mirroring and the shadow below)\n",
    "classes_of_interest = [5, 6]\n",
    "mask_copy = np.zeros_like(mask)\n",
    "for c in classes_of_interest:\n",
    "    mask_copy[mask == c] = 1\n",
    "\n",
    "# Get the shadow and set to intrested\n",
    "shadow_x, shadow_y = get_shadow(\n",
    "    mask, instrument_label=5, shadow_label=6, top_layer_label=1)\n",
    "mask_copy[shadow_x, shadow_y] = 1\n",
    "\n",
    "# Only instrument, mirroring and shadow are 1 now\n",
    "mask = torch.as_tensor(mask_copy)\n",
    "\n",
    "mask_0 = mask.unsqueeze(0)\n",
    "mask_1 = mask_0.clone()\n",
    "mask_1 = 1 - (mask_1)  # (1,h,w)\n",
    "\n",
    "# Here I set the target mask to the same as original mask\n",
    "targ_mask_0 = mask_0.clone()\n",
    "targ_mask_1 = mask_1.clone()\n",
    "\n",
    "mask_0 = mask_0.to(device)\n",
    "mask_1 = mask_1.to(device)\n",
    "targ_mask_0 = targ_mask_0.to(device)\n",
    "targ_mask_1 = targ_mask_1.to(device)\n",
    "\n",
    "cross_over_source = (img_source * mask_1) + (img_target * targ_mask_0)\n",
    "cross_over_target = (img_source * mask_0) + (img_target * targ_mask_1)\n",
    "\n",
    "# for visualization\n",
    "cross_over_source_image = tensor2image(cross_over_source.to('cpu'))  # [256, 256, n_channels(1)]\n",
    "cross_over_target_image = tensor2image(cross_over_target.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8852f27-0cbd-403e-9847-7e1a5052ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(5, 10))\n",
    "axes[0].imshow(cross_over_source_image)\n",
    "axes[1].imshow(cross_over_target_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad87868-6950-429e-b2a9-f8168d3750c4",
   "metadata": {},
   "source": [
    "### 5. Create a input latent and noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147eec9-8211-4946-aebc-051165b31d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ema = generator\n",
    "with torch.no_grad():\n",
    "    noise_sample = torch.randn(n_mean_latent, 512, device=device)\n",
    "    latent_out = g_ema.style(noise_sample)\n",
    "    latent_mean = latent_out.mean(0)\n",
    "    \n",
    "noises_single = g_ema.make_noise()\n",
    "noises = []\n",
    "for noise in noises_single:\n",
    "    noises.append(noise.repeat(img_source.shape[0], 1, 1, 1).normal_())\n",
    "\n",
    "# Create a new latent as optimization starting point\n",
    "latent_in = latent_mean.detach().clone().unsqueeze(\n",
    "    0).repeat(img_source.shape[0], 1)\n",
    "latent_in = latent_in.unsqueeze(1).repeat(1, g_ema.n_latent, 1)\n",
    "\n",
    "# Both latents and noise are optimized? (Only latent_in is in optimizer)\n",
    "latent_in.requires_grad = True\n",
    "for noise in noises:\n",
    "    noise.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0137908-62b7-457d-b147-4da450b1f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latent_in.shape)\n",
    "print(noises[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1521e-2eea-4478-b774-647523a41f8c",
   "metadata": {},
   "source": [
    "### 6. Initialize models\n",
    "\n",
    "Prepare models, optimizers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21195488-fa40-412e-aa07-bda26e65a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptual_net = VGG16_for_Perceptual(n_layers=[2, 4, 14, 21]).to(\n",
    "    device)  # conv1_1,conv1_2,conv2_2,conv3_3\n",
    "MSE_Loss = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam([latent_in], lr=lr)\n",
    "# TODO: manage size elsewhere\n",
    "mask_1 = mask_1.unsqueeze(0)\n",
    "mask_0 = mask_0.unsqueeze(0)\n",
    "\n",
    "loss_list = []\n",
    "latent_path = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b3a05f-4362-4afa-bad2-02195a0f58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_guided_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255798a9-113f-4b87-9c77-dcf4c350bbb1",
   "metadata": {},
   "source": [
    "### 7. Optimize the latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365f629-613d-4c69-bd42-9622b94ef82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_images = []\n",
    "\n",
    "for i in tqdm(10000):\n",
    "    t = i / mask_guided_iterations\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "\n",
    "    synth_img, _ = g_ema([latent_in], input_is_latent=True, noise=noises)\n",
    "\n",
    "    batch, channel, height, width = synth_img.shape\n",
    "\n",
    "    if height > image_size:\n",
    "        factor = height // image_size\n",
    "\n",
    "        synth_img = synth_img.reshape(\n",
    "            batch, channel, height // factor, factor, width // factor, factor\n",
    "        )\n",
    "        synth_img = synth_img.mean([3, 5])\n",
    "\n",
    "    loss_wl1 = caluclate_loss(synth_img, img_source,\n",
    "                              perceptual_net, mask_1, MSE_Loss, image_size)\n",
    "    loss_wl0 = caluclate_loss(synth_img, img_target,\n",
    "                              perceptual_net, mask_0, MSE_Loss, image_size)\n",
    "    mse_w0 = F.mse_loss(synth_img*mask_1.expand(1, 3, image_size, image_size),\n",
    "                        img_source*mask_1.expand(1, 3, image_size, image_size))\n",
    "    mse_w1 = F.mse_loss(synth_img*mask_0.expand(1, 3, image_size, image_size),\n",
    "                        img_target*mask_0.expand(1, 3, image_size, image_size))\n",
    "    mse_crossover = 3*(F.mse_loss(synth_img.float(),\n",
    "                       cross_over_source.float()))\n",
    "    p_loss = 2*((loss_wl0)+loss_wl1)\n",
    "    mse_loss = (mse_w0)+mse_w1\n",
    "    loss = (p_loss)+(mse_loss)+(mse_crossover)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    noise_normalize_(noises)\n",
    "\n",
    "    lr_schedule = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        latent_path.append(latent_in.detach().clone())\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "        loss_0 = loss_wl0.detach().cpu().numpy()\n",
    "        loss_1 = loss_wl1.detach().cpu().numpy()\n",
    "        mse_0 = mse_w0.detach().cpu().numpy()\n",
    "        mse_1 = mse_w1.detach().cpu().numpy()\n",
    "        mse_loss = mse_loss.detach().cpu().numpy()\n",
    "\n",
    "        print(\"iter{}: loss -- {:.5f},  loss0 --{:.5f},  loss1 --{:.5f}, mse0--{:.5f}, mse1--{:.5f}, mseTot--{:.5f}, lr--{:.5f}\".format(i,\n",
    "              loss_np, loss_0, loss_1, mse_0, mse_1, mse_loss, lr_schedule))\n",
    "\n",
    "        if save_iterations:\n",
    "            img_gen, _ = g_ema([latent_path[-1]],\n",
    "                               input_is_latent=True, noise=noises)\n",
    "            img_tens = (\n",
    "                img_gen.clamp_(-1., 1.).detach().squeeze().permute(1, 2, 0).cpu().numpy())*0.5 + 0.5\n",
    "            # pil_img = Image.fromarray((img_tens*255).astype(np.uint8))\n",
    "            # pil_img.save(img_name)\n",
    "            intermediate_images.append(img_tens)\n",
    "\n",
    "    if i == (mask_guided_iterations-1):\n",
    "        img_name = save_path_root+\"{}_D1.png\".format(str(i).zfill(6))\n",
    "        img_gen, _ = g_ema([latent_path[-1]],\n",
    "                           input_is_latent=True, noise=noises)\n",
    "        img_tens = (img_gen.clamp_(-1., 1.).detach().squeeze().permute(1,\n",
    "                    2, 0).cpu().numpy())*0.5 + 0.5\n",
    "        pil_img = Image.fromarray((img_tens*255).astype(np.uint8))\n",
    "        pil_img.save(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07195f-d412-417e-ac46-321e9fed9bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "n_col = 4\n",
    "n_row = math.ceil(len(intermediate_images) / n_col)\n",
    "plt.figure(figsize=(n_row * 3, n_col * 3))\n",
    "for i, img in enumerate(intermediate_images):\n",
    "    ax = plt.subplot(n_row, n_col, i+1)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8632679-f368-4072-9c53-1181a7e72164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
